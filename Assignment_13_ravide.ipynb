{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assigment_13_ravide.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "tootyIlp43q2",
        "TH0NaX7_4y7-",
        "wnkWel_14osO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9MWR1JfnAX9"
      },
      "source": [
        "# Assignment 13\n",
        "\n",
        "1. Gunakan kombinasi berikut untuk dataset wave:\n",
        "  - Feature extractor HOG dengan image size 128 dan 300\n",
        "  - Feature extractor LBP dengan image size 128 dan 300\n",
        "\n",
        "  Masing-masing kombinasi dilakukan training dengan 3 algoritma:\n",
        "    - LinearSVC\n",
        "    - Logistic Regression\n",
        "    - Random Forest\n",
        "  Gunakan random_state=10 apabila dibutuhkan!\n",
        "\n",
        "  Lakukan eksperimen terhadap hyperparameter sesuai kebutuhan dan tentukan komposisi mana yang dapat menghasilkan accuracy dan F1 score tertinggi! Berikan kesimpulan dan jelaskan!\n",
        "\n",
        "2. Lanjutkan untuk dataset wave:\n",
        "  - Ambilah secarik kertas dan buatlah 30 data tambahan untuk training dengan komposisi:\n",
        "    - 15 healthy (dengan gambar wave yang rapih)\n",
        "    - 15 parkinson (dengan gambar wave yang lebih ireguler)\n",
        "  - Lakukan training dengan komposisi feature extractor, ukuran gambar, algoritma, serta hyperparameter terbaik yang didapat untuk nomor 1!\n",
        "\n",
        "  Jelaskan, apakah terdapat perbedaan dalam hasil evaluasi training setelah ditambahkan data baru?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVRF8BT8OHc2"
      },
      "source": [
        "## Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQRweIzkdD0j"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from google.colab.patches import cv2_imshow\n",
        "from skimage import feature\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import cv2 as cv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tootyIlp43q2"
      },
      "source": [
        "## Fungsi Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYUFCP3edrrm"
      },
      "source": [
        "def preprocess(image, image_size=128):\n",
        "\t\timage = cv.cvtColor(image, cv.COLOR_BGR2GRAY) # Ubah mehjadi grayscale\n",
        "\t\timage = cv.resize(image, (image_size, image_size)) # Resize gambar menjadi suatu ukuran (default = 128)\n",
        "\n",
        "\t\timage = cv.threshold(image, 0, 255, cv.THRESH_BINARY_INV | cv.THRESH_OTSU)[1] # Melakukan thresholding dan mengambil hasil gambar thresholding\n",
        "\n",
        "\t\treturn image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH0NaX7_4y7-"
      },
      "source": [
        "## Fungsi Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdgxGdUGxbDL"
      },
      "source": [
        "# fungsi mengekstrak feature metode 'hog'\n",
        "def quantify_image_hog(image): # Histogram of Oriented Gradient (hog) features\n",
        "\tfeatures = feature.hog(image, orientations=9, pixels_per_cell=(10, 10), cells_per_block=(2, 2), transform_sqrt=True, block_norm=\"L1\")\n",
        "\n",
        "\treturn features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIVoTbB90o-g"
      },
      "source": [
        "# fungsi mengekstrak feature metode 'lbp'\n",
        "def quantify_image_lbp(image): # Local Binary Pattern  (lbp) features\n",
        "  features = feature.local_binary_pattern(image, 24, 8, method=\"uniform\")\n",
        "\n",
        "  (hist, _) = np.histogram(features.flatten(), bins=np.arange(0, 27), range=(0, 26))\n",
        "\n",
        "  hist = hist.astype(\"float\")\n",
        "  hist /= (hist.sum() + 1e-7)\n",
        "\n",
        "  return hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnkWel_14osO"
      },
      "source": [
        "## Fungsi Split Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lbj7IVlM2Rwu"
      },
      "source": [
        "# fungsi memisahkan dataset 'data' dan 'label'\n",
        "def load_split(path, image_size=200, extraction_method='hog'):\n",
        "  image_paths = list(paths.list_images(path))\n",
        "  data = []\n",
        "  labels = []\n",
        "\n",
        "  for image_path in image_paths:\n",
        "    label = image_path.split(os.path.sep)[-2]\n",
        "\n",
        "    image = cv.imread(image_path)\n",
        "    image = preprocess(image, image_size=image_size)\n",
        "\n",
        "    if extraction_method == 'hog':\n",
        "      features = quantify_image_hog(image)\n",
        "    elif extraction_method == 'lbp':\n",
        "      features = quantify_image_lbp(image)\n",
        "\n",
        "    data.append(features)\n",
        "    labels.append(label)\n",
        "\n",
        "  return (np.array(data), np.array(labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIDkNNIk5gRF"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0khuXGM5Y5D"
      },
      "source": [
        "# mengunggah dataset\n",
        "dataset_dir = 'drive/My Drive/BARU/wave'\n",
        "train_path = os.path.join(dataset_dir, 'training')\n",
        "test_path = os.path.join(dataset_dir, 'testing')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub9zGjbWIMLA"
      },
      "source": [
        "## Melatih dan Evaluasi Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7Fyny3lDf5C"
      },
      "source": [
        "# Library Pelatihan dan Evaluasi Model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7J0_60HCnqT"
      },
      "source": [
        "# fungsi image classification\n",
        "\n",
        "def image_class(extraction, resize):\n",
        "  # membagi feature dan label pada data training\n",
        "  (feature_train, label_train) = load_split(train_path, image_size=resize, \n",
        "                                          extraction_method=extraction)\n",
        "\n",
        "  # membagi feature dan label pada data testing\n",
        "  (feature_test, label_test) = load_split(test_path, image_size=resize,\n",
        "                                        extraction_method=extraction)\n",
        "  \n",
        "  # Label Encoder\n",
        "  laben = LabelEncoder()\n",
        "  label_train = laben.fit_transform(label_train)\n",
        "  label_test = laben.fit_transform(label_test)\n",
        "\n",
        "  # Model Linear SVC\n",
        "  model_lsvc = LinearSVC(loss='squared_hinge', C=100)\n",
        "  model_lsvc.fit(feature_train, label_train)\n",
        "  lebel_pred_lsvc = model_lsvc.predict(feature_test)\n",
        "\n",
        "  # Model Logistic Regression\n",
        "  model_lr = LogisticRegression()\n",
        "  model_lr.fit(feature_train, label_train)\n",
        "  label_pred_lr = model_lr.predict(feature_test)\n",
        "\n",
        "  # Model Random Forest Classifier\n",
        "  model_rfc = RandomForestClassifier(n_estimators=1000, criterion='entropy', \n",
        "                                     max_depth=5, max_features='auto', \n",
        "                                     random_state=10)\n",
        "  model_rfc.fit(feature_train, label_train)\n",
        "  label_pred_rfc = model_rfc.predict(feature_test)\n",
        "\n",
        "  nama_model = ['Linear SVC', 'Logistic Regression', 'Random Forest Classifier']\n",
        "  model = [model_lsvc, model_lr, model_rfc]\n",
        "  label_pred = [lebel_pred_lsvc, label_pred_lr, label_pred_rfc]\n",
        "\n",
        "  # Evaluasi model\n",
        "  for i, label in enumerate(label_pred):\n",
        "    print('Nilai Akurasi data test {}'.format(nama_model[i]))\n",
        "    print('- Accuracy \\t: {:.2f}'.format(model[i].score(feature_test, label_test)))\n",
        "    print('- F1 \\t\\t: {:.2f}'.format(f1_score(label_test, label, average='macro')))\n",
        "    print('- Precission \\t: {:.2f}'.format(precision_score(label_test, label, average='macro')))\n",
        "    print('- Recall \\t: {:.2f}'.format(recall_score(label_test, label, average='macro')))\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NfABPOhW4pq"
      },
      "source": [
        "## Jawaban No. 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA9-3JIkOp_M"
      },
      "source": [
        "### HOG 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjwpnxqxF7ha",
        "outputId": "60a8400e-1d57-458a-a784-f754e5ecd026",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "# Feature extractor HOG dengan image size 128\n",
        "image_class('hog', 128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nilai Akurasi data test Linear SVC\n",
            "- Accuracy \t: 0.67\n",
            "- F1 \t\t: 0.67\n",
            "- Precission \t: 0.67\n",
            "- Recall \t: 0.67\n",
            "\n",
            "\n",
            "Nilai Akurasi data test Logistic Regression\n",
            "- Accuracy \t: 0.67\n",
            "- F1 \t\t: 0.67\n",
            "- Precission \t: 0.67\n",
            "- Recall \t: 0.67\n",
            "\n",
            "\n",
            "Nilai Akurasi data test Random Forest Classifier\n",
            "- Accuracy \t: 0.73\n",
            "- F1 \t\t: 0.73\n",
            "- Precission \t: 0.74\n",
            "- Recall \t: 0.73\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pch5cdYWO208"
      },
      "source": [
        "### HOG 300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6A6a423GBhH",
        "outputId": "1158bd3f-abe2-4e35-96c3-675683d4d12a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "# Feature extractor HOG dengan image size 300\n",
        "image_class('hog', 300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nilai Akurasi data test Linear SVC\n",
            "- Accuracy \t: 0.73\n",
            "- F1 \t\t: 0.73\n",
            "- Precission \t: 0.74\n",
            "- Recall \t: 0.73\n",
            "\n",
            "\n",
            "Nilai Akurasi data test Logistic Regression\n",
            "- Accuracy \t: 0.70\n",
            "- F1 \t\t: 0.70\n",
            "- Precission \t: 0.70\n",
            "- Recall \t: 0.70\n",
            "\n",
            "\n",
            "Nilai Akurasi data test Random Forest Classifier\n",
            "- Accuracy \t: 0.73\n",
            "- F1 \t\t: 0.73\n",
            "- Precission \t: 0.73\n",
            "- Recall \t: 0.73\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0qAOyLJO8d7"
      },
      "source": [
        "### LBP 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8a5lbwLGCPP",
        "outputId": "0ff0c038-c9f7-4c36-b379-69c6387cc67e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "# # Feature extractor LBP dengan image size 128\n",
        "image_class('lbp', 128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Nilai Akurasi data test Linear SVC\n",
            "- Accuracy \t: 0.57\n",
            "- F1 \t\t: 0.55\n",
            "- Precission \t: 0.57\n",
            "- Recall \t: 0.57\n",
            "\n",
            "\n",
            "Nilai Akurasi data test Logistic Regression\n",
            "- Accuracy \t: 0.50\n",
            "- F1 \t\t: 0.50\n",
            "- Precission \t: 0.50\n",
            "- Recall \t: 0.50\n",
            "\n",
            "\n",
            "Nilai Akurasi data test Random Forest Classifier\n",
            "- Accuracy \t: 0.53\n",
            "- F1 \t\t: 0.53\n",
            "- Precission \t: 0.53\n",
            "- Recall \t: 0.53\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo--koL6PCsE"
      },
      "source": [
        "### LBP 300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yhg8a10uGCvI",
        "outputId": "e730a8c5-a9fb-4cad-e4d5-ed5087f0a219",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "# Feature extractor LBP dengan image size 300\n",
        "image_class('lbp', 300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Nilai Akurasi data test Linear SVC\n",
            "- Accuracy \t: 0.57\n",
            "- F1 \t\t: 0.56\n",
            "- Precission \t: 0.57\n",
            "- Recall \t: 0.57\n",
            "\n",
            "\n",
            "Nilai Akurasi data test Logistic Regression\n",
            "- Accuracy \t: 0.53\n",
            "- F1 \t\t: 0.53\n",
            "- Precission \t: 0.53\n",
            "- Recall \t: 0.53\n",
            "\n",
            "\n",
            "Nilai Akurasi data test Random Forest Classifier\n",
            "- Accuracy \t: 0.70\n",
            "- F1 \t\t: 0.70\n",
            "- Precission \t: 0.70\n",
            "- Recall \t: 0.70\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j7GZk1MZi-o"
      },
      "source": [
        "## Kesimpulan:\n",
        "- Dari tunning parameter yang telah dilakukan maka dipreoleh lah akurasi yang terbaik yaitu model linear SVC dengan metode feature extractor HOG dan ukuran gambar 300 dengan nilai akurasi = 73, f1_score = 73, precision = 74, dan recall 73 \n",
        "- Dapat dilihat bahwa pada metode feature extractor HOG akan menghasilkan akurasi yang lebih bagus jika ukuran size ditingkatkan karena dengan bertambah banyak feature2 yang diekstraksi dari HOG semakin mudah untuk mengklasifikasi gambar.\n",
        "- begitu juga sebaliknya dengan metode extractor LBP, ukuran size dinaikkan akurasinya juga bertambah namun belum dapat sebaik akurasi extractor HOG karena feature2 pad HOG lebih baik dalam mengklasifikasi gambar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTbbWkAMzfCM"
      },
      "source": [
        "#Jawaban No. 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG-JHiKtbK4O"
      },
      "source": [
        "# mengunggah dataset yang sudah ditambah data baru\n",
        "dataset_dir = 'drive/My Drive/assigment/wave'\n",
        "train_path = os.path.join(dataset_dir, 'training')\n",
        "test_path = os.path.join(dataset_dir, 'testing')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf0LBXpmbwN2",
        "outputId": "652e28ce-6ff5-4f71-f0da-5191eb3d45c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "# Algoritma (linear SVC), feature extractor (HOG) dan size (300) terbaik no. 1\n",
        "# Feature extractor HOG dengan image size 300\n",
        "image_class('hog', 300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nilai Akurasi data test Linear SVC\n",
            "- Accuracy \t: 0.63\n",
            "- F1 \t\t: 0.63\n",
            "- Precission \t: 0.63\n",
            "- Recall \t: 0.63\n",
            "\n",
            "\n",
            "Nilai Akurasi data test Logistic Regression\n",
            "- Accuracy \t: 0.60\n",
            "- F1 \t\t: 0.60\n",
            "- Precission \t: 0.60\n",
            "- Recall \t: 0.60\n",
            "\n",
            "\n",
            "Nilai Akurasi data test Random Forest Classifier\n",
            "- Accuracy \t: 0.73\n",
            "- F1 \t\t: 0.73\n",
            "- Precission \t: 0.74\n",
            "- Recall \t: 0.73\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li1-4Lm5nXyw",
        "outputId": "47628572-f608-4d9b-de44-1d46b2f599d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "# Dibandingkan dengan ukuran size yang lebih kecil\n",
        "# Feature extractor HOG dengan image size 100\n",
        "image_class('hog', 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nilai Akurasi data test Linear SVC\n",
            "- Accuracy \t: 0.73\n",
            "- F1 \t\t: 0.73\n",
            "- Precission \t: 0.75\n",
            "- Recall \t: 0.73\n",
            "\n",
            "\n",
            "Nilai Akurasi data test Logistic Regression\n",
            "- Accuracy \t: 0.80\n",
            "- F1 \t\t: 0.80\n",
            "- Precission \t: 0.81\n",
            "- Recall \t: 0.80\n",
            "\n",
            "\n",
            "Nilai Akurasi data test Random Forest Classifier\n",
            "- Accuracy \t: 0.80\n",
            "- F1 \t\t: 0.80\n",
            "- Precission \t: 0.80\n",
            "- Recall \t: 0.80\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WxTRgS80wG2"
      },
      "source": [
        "# lokasi data yng ditambah dan belum ditambah\n",
        "dir_b = 'drive/My Drive/assigment/wave/training/healthy'\n",
        "dir_a = 'drive/My Drive/BARU/wave/training/healthy'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4AQTSQv0_03",
        "outputId": "ac59c08a-0d01-4572-a08d-a7cc9ebed18a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# jumlah data sesudah ditambah\n",
        "len(os.listdir(dir_b))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs3NKDWQ1lFs",
        "outputId": "4e774838-4973-4e44-9bf2-4047ed0f1dc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# jumlah data sebelum ditambah\n",
        "len(os.listdir(dir_a))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp1gK8L4mk90"
      },
      "source": [
        "## Kesimpulan\n",
        "- Data yang saya tambah secara manual\n",
        "- Dengan komposisi feature extractor, ukuran gambar, algoritma, serta hyperparameter terbaik yang didapat untuk nomor 1 (algoritma Linear SVC, extractor HOG dan size 300), akurasi pada no. 2 malah berkurang.\n",
        "- Hal ini disebakan karena semakin banyak data yang ditambah ke data training sebelumnya semakin banyak pula komputer belajar sehingga memberikan pattern baru dan dengan pattern yang baru tersebut belum dapat mengklasifikasi data testing dengan lebih baik ddari sebelumnya.\n",
        "- Namun saat ukuran size pada extractor HOG dikecilkan menjadi 100, hasil akurasinya menjadi lebih meningkat.\n",
        "- Dapat disimpulkan bahwa saat dataset ditambah pada data training dengan jumlah feature yang banyak (HOG size 300) maka komputer akan semakin komplek mengklasifikasi gambar namun saat featurenya disedikitkan (HOG size 100) maka setiap pola feature pada gambar semaikn jelas dan mempermudah untuk mengklasifikasi gambar.\n",
        "- Dengan bertambah data training belum tentu meningkatkan akurasi selain juga harus diiringi jumlah feature2 pada gambar yang saling berkolerasi sehingga jika menambah dataset maka ukuran sizenya juga harus diatur agar mmendapatkan pattern yang sesuai dan juga mengatur hyperparameter model algoritma. "
      ]
    }
  ]
}